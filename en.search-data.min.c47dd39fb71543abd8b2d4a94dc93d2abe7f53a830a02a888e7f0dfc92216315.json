[{"id":0,"href":"/docs/tech/airbyte/installation/","title":"Installation","section":"Airbyte","content":" Airbyte 설치 방법 # 해당 문서에서는 docker-compose를 사용하여 airbyte를 설치하는 방법에 대해서 설명합니다.\n선행조건 # docker 및 docker-compose 설치 git 설치(sudo apt-get install git) Airbyte 다운로드 # git clone https://github.com/airbytehq/airbyte.git 스크립트 실행 # Airbyte에서는 설치 및 배포 스크립트를 제공합니다. docker-compose 및 .env (Airbyte 설정 파일) 을 다운로드 합니다. options: -d --download Only download files - don\u0026#39;t run docker compose -r --refresh DELETE existing assets and re-download new ones -h --help Print this Help. -x --debug Verbose mode. -b --background Run docker compose up in detached mode. cd airbyte ./run-ab-platform.sh -d Airbyte 환경 설정 # docker-compose 실행 이전, 운영하려는 환경에 대한 설정을 진행합니다. Core, Secrets(Vault, GCP Secret store, AWS KMS), Database, Airbyte Service, Jobs, Connection, Logging, Monitoring, Worker, Launcher, Data Retention, docker, k8s 옵션 등 다양하게 존재하기 때문에 Reference을 보면서 설정하는 것이 좋습니다. https://docs.airbyte.com/operator-guides/configuring-airbyte docker-compose 실행 # docker compose up -d (./run-ab-platform.sh -b) Airbyte 접속 # http://localhost:8000/ Reference # https://docs.airbyte.com/operator-guides/configuring-airbyte "},{"id":1,"href":"/docs/tech/js7-jobscheduler/introduce/","title":"Introduce","section":"JS7 JobScheduler","content":" JS7 JobScheduler Controller and Agent 개요 # JS7 JOC Cockpit Dashboard UI\n개요 # 홈페이지(회사): https://www.sos-berlin.com/en Documentation: https://kb.sos-berlin.com/display/JS7/JS7 이슈 추적: https://change.sos-berlin.com/secure/Dashboard.jspa 포럼: https://sourceforge.net/p/jobscheduler/discussion/ 라이센스 정책 # JS7 JobScheduler와 YADE 제품은 고객에게 오픈 소스 라이선스와 상업용 라이선스 중에서 선택할 수 있는 듀얼 라이선스 모델로 제공 오픈 소스 라이선스 GPLv3(일반 공중 라이선스)에 따라 제공 SOS의 상용 라이센스 구매 유일한 예외는 기업 고객을 위한 상업적으로 이용 가능한 기능인 고가용성을 위해 JS7 제품을 클러스터링하는 운영 기능 초기 해당 제품을 사용하여 피봇팅을 하는 경우 단일 컨트롤러에 독립형 에이전트를 여러개 사용할 수 있으나, 규모가 확장되고 SPOF를 피하는 목적으로 에이전트 클러스터링을 고려할 수 있음 아키텍처 구성요소 # Controller 컨트롤러는 실행할 워크플로와 작업, 실행 시기, 실행에 사용할 에이전트를 알고 있습니다. 컨트롤러는 JOC Cockpit에서 작업 관련 인벤토리를 수신하고 이 정보를 각 서버에서 워크플로우와 작업을 실행하는 에이전트에 배포합니다. 독립형 컨트롤러는 에이전트를 오케스트레이션하는 단일 인스턴스입니다. 할당하는 에이전트의 개수는 무제한입니다. Agent 에이전트은 에이전트 서버에서 실행 파일 및 명령을 호출하는 작업을 실행합니다. 에이전트는 컨트롤러로부터 시작할 잡과 시작 시점에 대한 정보를 받습니다. 에이전트는 실행 결과와 로그 출력을 컨트롤러에 다시 보고합니다. 에이전트는 작업 실행 시점에 컨트롤러가 연결되지 않아도 자율적으로 작동할 수 있습니다. 독립형 에이전트는 서로 독립적으로 작동합니다. JOC Cockpit JOC Cockpit은 작업 관련 인벤토리를 관리하고 컨트롤러 및 에이전트의 워크플로 실행을 모니터링 및 제어하기 위한 사용자 인터페이스입니다. 독립형 JOC Cockpit은 하나 이상의 컨트롤러를 관리하는 데 사용할 수 있는 단일 인스턴스입니다. "},{"id":2,"href":"/docs/tech/airbyte/mysql-to-snowflake/","title":"Airbyte를 사용하여 MySQL to Snowflake 연결 시 고려 사항","section":"Airbyte","content":" Airbyte를 사용하여 MySQL to Snowflake 연결 시 고려 사항 # 개요 # EC2 내부에 docker-compose를 사용하여 Airbyte를 생성하고, Source는 AWS RDS 위에 운영되는 MySQL, Destination은 Snowflake로 각각 커넥터 설정을 하였습니다. 환경을 구성 후 CDC를 진행하며 겪었던 상황과 해결 방법을 작성하였습니다.\n1. MySQL에서 1억건 이상의 테이블에 대한 초기 마이그래이션 시 리소스 고려 # Airbyte는 CDC 이전 초기 연결 시 테이블에 대해 Source에서 Destination으로 마이그래이션을 진행합니다. 마이그래이션 시 RDS 리소스도 중요하지만 EC2 리소스를 더 많이 사용합니다. RDS는 Task 당 커넥션을 하나만 사용하지만, EC2는 돌아가고 있는 모든 Task들을 관리해야 하기 때문입니다.\n1억건 이상 테이블 당 마이그래이션 시 RDS 리소스는 db.m6i.2xlarge 기준 CPU 점유율이 2% 정도 상승하였습니다. 반면 EC2는 c7i.4xlarge 기준 CPU 점유율이 20% 되었습니다. 그래서 EC2 c7i.4xlarge 기준 1억건 이상 테이블을 5개 이상 한번에 적재하는 경우 CPU 리소스가 부족할 수 있습니다. 그래서 초기 마이그래이션 시 테이블 하나당 EC2 c7i.xlarge 기준으로 생각하여 고려하고 있는 병렬 처리 테이블 개수만큼 인스턴스 크기를 늘려서 짧은 시간에 마이그래이션을 완료할 수 있도록 합니다. CPU 리소스가 부족한 경우, Sync 과정에서 중지가 될 수 있으며 지금까지 받은 데이터를 모두 날리고 새로 받아야 하는 경우도 생길 수 있습니다.\n2. CDC를 위한 RDS MySQL 설정 수정 # 시스템에서 사용하는 DB에는 별도 설정없이 기본값으로 사용하는 경우가 있습니다. 그런 경우 MySQL에서 복제, 동기화 등의 목적으로 사용하는 binlog 에 대한 설정도 기본값인 경우가 대부분입니다.\nbinlog 설정 시 확인했던 부분은 다음과 같습니다.\nbinlog 확인: SHOW binary logs; RDS 내부 설정 확인: CALL mysql.rds_show_configuration; 설정 확인 후 MySQL에서 binlog를 수정한 내용은 다음과 같습니다.\nRDS 파라미터 그룹 수정 RDS 파라미터 그룹은 MySQL 중지없이 설정 변경 시 즉시 적용되어 binlog 관련 설정을 운영에 지장없이 변경할 수 있었습니다. binlog_format: MIXED -\u0026gt; ROW (CDC Required) binlog_row_image: null(full) -\u0026gt; full (CDC Required) MySQL 내부에서 프로시저 실행 해당 프로시저 또한 MySQL 중지없이 실행으로 설정을 변경할 수 있었습니다. CALL mysql.rds_set_configuration('binlog retention hours', 25); binlog에 대한 retention 시간을 25시간으로 두었기 때문에, 25시간 이상 간격이 필요한 CDC에는 설정값을 높힐 필요가 있습니다. 3. 초기 MySQL에서 Snowflake 테이블 적재 시 대략적인 소요 시간 # 적재하는 Source나 Destination의 종류, Airbyte가 서비스되는 플랫폼의 종류 및 인프라 제원 등에 따라서 마이그래이션 시간은 상이할 수 있습니다. 이 부분을 참고하셔서 대략적으로 참고하실 수 있도록 테이블 마이그래이션 및 CDC 결과 공유드립니다.\n공통 사항 # EC2 인스턴스 크기: c7i.4xlarge 테이블 1 # 용량: 226.8GB ROW: 약 430,000,000 (430M) 마이그래이션 시간: 약 7시간 CDC 시간: 평균 4~5만 Row 당 4분 소요 CDC 시 12~13% 소모 테이블 2 # 용량: 31.7GB ROW: 약 42,000,000 마이그래이션 시간: 약 48분 "},{"id":3,"href":"/docs/tech/js7-jobscheduler/installation/","title":"Installation","section":"JS7 JobScheduler","content":" JS7 JobScheduler Controller and Agent 설치 (w/ docker-compse) # 1. 공식 문서 참조 기준 # 현재 Alpine 기본 이미지와 OpenJDK와 함께 제공되는 Linux 기반 OCI 호환 컨테이너 이미지 에서 제공\n2025.01.03 기준 2.5.0 버전으로 공식 문서에 적혀있어서 참조하였습니다.\n1-1. dotenv 설정 # docker-compose 내 설정 값으로 들어가야 하는 항목들을 지정합니다. JS7USERID=1000 JS7GROUPID=0 JS7VERSION=2-5-0 1-2. Agent # docker-compose.yml 을 다운로드 받습니다. agent의 Volume이 마운트 되는 폴더를 생성합니다. mkdir js7-agent-primary docker-compose를 실행합니다. dotenv를 적용해야 합니다. docker compose --env-file ./.env -f docker-compose.yml up -d docker-compose.yml version: '3' services: js7-agent-primary: image: sosberlin/js7:agent-${JS7VERSION} hostname: js7-agent-primary volumes: - js7-agent-primary:/var/sos-berlin.com/js7/agent/var_4445 networks: - js7 environment: RUN_JS_JAVA_OPTIONS: -Xmx256m RUN_JS_USER_ID: \"${JS7USERID}:${JS7GROUPID}\" restart: \"no\" networks: js7: external: true volumes: js7-agent-primary: driver: local driver_opts: type: none device: ${PWD}/js7-agent-primary o: bind 1-3. Controller # docker-compose.yml 을 다운로드 받습니다. controller의 Volume이 마운트 되는 폴더를 생성합니다. mkdir js7-controller-primary docker-compose를 실행합니다. dotenv를 적용해야 합니다. docker compose --env-file ./.env -f docker-compose.yml up -d docker-compose.yml version: \u0026#39;3\u0026#39; services: js7-controller-primary: image: sosberlin/js7:controller-${JS7VERSION} hostname: js7-controller-primary volumes: - js7-controller-primary:/var/sos-berlin.com/js7/controller/var networks: - js7 environment: RUN_JS_JAVA_OPTIONS: -Xmx256m RUN_JS_USER_ID: \u0026#34;${JS7USERID}:${JS7GROUPID}\u0026#34; restart: \u0026#34;no\u0026#34; networks: js7: external: true volumes: js7-controller-primary: driver: local driver_opts: type: none device: ${PWD}/js7-controller-primary o: bind o: bind 1-4. JOC Cockpit # MySQL, PostgreSQL 두가지 종류의 DB를 지원합니다. (이 중 MySQL을 사용한 설치 예시를 작성하였습니다.)\ndocker-compose.yml 및 hibernate.cfg.xml 을 다운로드 받습니다. JOC Cockpit의 Volume이 마운트 되는 폴더를 생성합니다. mkdir db_data mkdir js7-joc-primary-config mkdir js7-joc-primary-logs docker-compose를 실행합니다. dotenv를 적용해야 합니다. docker compose --env-file ./.env -f docker-compose.yml up -d hibernate.cfg.yml을 js7-joc-primary-config 폴더로 복사합니다. cp -f hibernate.cfg.xml js7-joc-primary-config/ JS7 Scheduler의 메타 정보를 저장하는 MySQL을 초기화합니다. 초기화 진행 docker-compose exec js7-joc-primary /bin/sh -c /opt/sos-berlin.com/js7/joc/install/joc_install_tables.sh 결과 확인 tail js7-joc-primary-logs/install-result.log docker-compose.yml version: \u0026#39;3\u0026#39; services: db: image: mysql:8.0 command: --default-authentication-plugin=mysql_native_password volumes: - db_data:/var/lib/mysql ports: - \u0026#34;3306:3306\u0026#34; networks: - js7 environment: MYSQL_ROOT_PASSWORD: js7rootpassword MYSQL_DATABASE: js7db MYSQL_USER: js7user MYSQL_PASSWORD: js7password restart: \u0026#34;no\u0026#34; js7-joc-primary: depends_on: - db container_name: js7-joc-primary image: sosberlin/js7:joc-${JS7VERSION} hostname: js7-joc-primary ports: - \u0026#34;17446:4446\u0026#34; networks: - js7 volumes: - js7-joc-primary-config:/var/sos-berlin.com/js7/joc/resources/joc - js7-joc-primary-logs:/var/sos-berlin.com/js7/joc/logs environment: RUN_JS_JAVA_OPTIONS: -Xmx256m RUN_JS_USER_ID: \u0026#34;${JS7USERID}:${JS7GROUPID}\u0026#34; restart: \u0026#34;no\u0026#34; networks: js7: external: true volumes: db_data: driver: local driver_opts: type: none device: ${PWD}/db_data o: bind js7-joc-primary-config: driver: local driver_opts: type: none device: ${PWD}/js7-joc-primary-config o: bind js7-joc-primary-logs: driver: local driver_opts: type: none device: ${PWD}/js7-joc-primary-logs o: bind hibernate.cfg.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; standalone=\u0026#34;no\u0026#34;?\u0026gt; \u0026lt;hibernate-configuration\u0026gt; \u0026lt;session-factory\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.driver_class\u0026#34;\u0026gt;org.mariadb.jdbc.Driver\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.password\u0026#34;\u0026gt;js7password\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.url\u0026#34;\u0026gt;jdbc:mariadb://db:3306/js7db\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.username\u0026#34;\u0026gt;js7user\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.dialect\u0026#34;\u0026gt;org.hibernate.dialect.MySQLInnoDBDialect\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.show_sql\u0026#34;\u0026gt;false\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.autocommit\u0026#34;\u0026gt;false\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.format_sql\u0026#34;\u0026gt;true\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.temp.use_jdbc_metadata_defaults\u0026#34;\u0026gt;false\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.provider_class\u0026#34;\u0026gt;org.hibernate.hikaricp.internal.HikariCPConnectionProvider\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.hikari.maximumPoolSize\u0026#34;\u0026gt;10\u0026lt;/property\u0026gt; \u0026lt;/session-factory\u0026gt; \u0026lt;/hibernate-configuration\u0026gt; Reference # {% embed url=\u0026ldquo; https://kb.sos-berlin.com/display/JS7/JS7+-+Installation+for+Containers?src=contextnavpagetreemode\" %}\n{% embed url=\u0026ldquo; https://qiita.com/saitamanokusa/items/ffb8f05cbc8e75d435ce\" %}\n2. 설치 개선 # 공식 문서에는 2.5.0 버전으로 설치를 진행하게 되어 있지만, 최근까지 계속 메인테이너가 개발을 하고 있고 Stable Version이 2.7.3이여서 해당 버전으로 개선안을 작성했습니다.\n2-1. Agent 컨테이너에 Python 환경 구성 # 기존 빌드된 컨테이너에는 Python 환경이 구성되지 않았습니다. Python을 실행하기 위해서 기존 이미지를 바탕으로 새로운 이미지를 빌드해야 했습니다. Python과 의존성을 가지는 환경을 Dockerfile로 구성했습니다. Python 환경, 파일들을 공유할 폴더도 생성하도록 추가했습니다.\nPython은 UV 기반으로 구성할 예정이여서 설치 스크립트가 추가되어 있습니다. 사용하는 환경에 따라서 Dockerfile 구성을 변경하면 됩니다.\nFROM sosberlin/js7:agent-2-7-3 RUN apk add \\ wget \\ gcc \\ make \\ zlib-dev \\ libffi-dev \\ openssl-dev \\ musl-dev RUN apk add --no-cache python3 python3-dev py3-pip RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y ENV PATH=\"$PATH:/root/.cargo/bin\" RUN curl -LsSf https://astral.sh/uv/install.sh | sh ENV PATH=\"$PATH:/root/.local/bin\" RUN mkdir /var/sos-berlin.com/js7/agent/workspace 파이썬 사용 시 주의사항 # JS7 JobScheduler에서 사용한 컨테이너는 Alpine Linux 를 사용했습니다. Alpine Linux는 C 컴파일에 musl을 사용하기 때문에 Python의 특정 라이브러리를 설치 시 binary wheel 을 사용하지 못합니다. 그래서 설치시 C 코드를 컴파일해야 하는 문제점이 발생합니다. 해당 문제 때문에 설치 시간이 기존 보다 몇 (십)배는 느려지는 이슈가 있습니다. 그래서 라이브러리 사용에 제약 사항이 발생할 수 있습니다.\n현재 확인한 설치가 느린 라이브러리는 Polars, oracledb 가 있습니다. {% embed url=\u0026ldquo; https://eden-do.tistory.com/64\" %}\n{% embed url=\u0026ldquo; https://nx006.tistory.com/70\" %}\n2-2. docker-compose 통합 및 Agent 다중화 # 문서에는 Agent, Controller, JOC를 각각의 docker-compose 파일로 분리했었습니다. 개발 환경을 구성할 떄 편리함을 위해서 한 개의 docker-compose 파일로 통합하였습니다. 그리고 Enterprise 버전을 사용하지 않더라도 여러 개의 Standalone Agent를 Controller에 등록할 수 있습니다. 그래서 여러 개의 Agent를 사용하도록 deploy 옵션을 추가했습니다. Replicas 옵션은 dotenv에서 수정할 수 있도록 하였습니다. 그리고 각 Agent가 실행할 코드를 공유할 수 있도록 docker volume를 추가하였습니다. Python 코드를 Git Repo에서 Pull 방식으로 배포 후 Agent 에서 사용할 수 있도록 하기 위해 해당 방법으로 구성 하였습니다.. Rolling이나 Blue/Green 등배포 전략을 사용하기에는 Agent 를 Controller에서 등록하는 방식때문에 배포 전략을 사용하기 어려운 점도 고려했습니다.\ndotenv # JS7USERID=1000 JS7GROUPID=0 JS7VERSION=2-7-3 JS7_AGENT_REPLICAS=3 docker-compose.yml # version: \u0026#39;3\u0026#39; services: db: image: mysql:8.0 ports: - \u0026#34;3306:3306\u0026#34; command: - --default-authentication-plugin=caching_sha2_password - --character-set-server=utf8mb4 - --collation-server=utf8mb4_bin - --skip-character-set-client-handshake volumes: - db_data:/var/lib/mysql networks: - js7 environment: MYSQL_ROOT_PASSWORD: iHateMySQL8.0! MYSQL_DATABASE: js7db MYSQL_USER: js7user MYSQL_PASSWORD: js7password restart: \u0026#34;no\u0026#34; js7-joc-primary: depends_on: - db image: sosberlin/js7:joc-${JS7VERSION} hostname: js7-joc-primary ports: - \u0026#34;17443:4446\u0026#34; volumes: - js7-joc-primary-config:/var/sos-berlin.com/js7/joc/resources/joc - js7-joc-primary-logs:/var/log/sos-berlin.com/js7/joc networks: - js7 environment: RUN_JS_JAVA_OPTIONS: -Xmx256m RUN_JS_USER_ID: \u0026#34;${JS7USERID}:${JS7GROUPID}\u0026#34; restart: \u0026#34;no\u0026#34; js7-controller-primary: image: sosberlin/js7:controller-${JS7VERSION} hostname: js7-controller-primary volumes: - js7-controller-primary:/var/sos-berlin.com/js7/controller networks: - js7 environment: RUN_JS_JAVA_OPTIONS: -Xmx256m RUN_JS_USER_ID: \u0026#34;${JS7USERID}:${JS7GROUPID}\u0026#34; restart: \u0026#34;no\u0026#34; js7-agent-primary: build: context: . dockerfile: Dockerfile.js7-agent entrypoint: entrypoint.sh hostname: js7-agent-primary volumes: - js7-agent-primary-workspace:/var/sos-berlin.com/js7/agent/workspace networks: - js7 environment: RUN_JS_JAVA_OPTIONS: -Xmx256m RUN_JS_USER_ID: \u0026#34;${JS7USERID}:${JS7GROUPID}\u0026#34; restart: \u0026#34;no\u0026#34; deploy: mode: replicated replicas: ${JS7_AGENT_REPLICAS} endpoint_mode: vip volumes: db_data: driver: local driver_opts: type: none device: ${PWD}/db_data o: bind js7-joc-primary-config: driver: local driver_opts: type: none device: ${PWD}/js7-joc-primary-config o: bind js7-joc-primary-logs: driver: local driver_opts: type: none device: ${PWD}/js7-joc-primary-logs o: bind js7-controller-primary: driver: local driver_opts: type: none device: ${PWD}/js7-controller-primary o: bind js7-agent-primary-workspace: driver: local driver_opts: type: none device: ${PWD}/js7-agent-primary-workspace o: bind networks: js7: 2-3. JS7 JobScheduler 초기 개발 환경 구성 스크립트 작성 # 일련의 작업 과정을 자동화하기 위해 스크립트를 작성하였습니다. docker-compose up 이후 20초 정도 기다렸다가 DB 초기화 작업을 진행합니다. 만약 DB 초기화 중 에러가 발생하였다면, 스크립트의 마지막 두 줄을 다시 실행하여야 합니다. 초기화가 완료되지 않는다면, JOC가 열리지 않습니다. mkdir js7-agent-workspace mkdir js7-controller-primary mkdir js7-joc-primary-config mkdir js7-joc-primary-logs mkdir db_data docker compose --env-file ./.env -f docker-compose.yml up -d echo \u0026#34;Waiting for the database to start...\u0026#34; sleep 20 cp -f hibernate.cfg.xml js7-joc-primary-config/ docker-compose exec js7-joc-primary /bin/bash -c /opt/sos-berlin.com/js7/joc/install/joc_install_tables.sh docker-compose restart js7-joc-primary "},{"id":4,"href":"/docs/tech/airbyte/disk-issue/","title":"Airbyte 운용 시 디스크 용량이 많이 찼을 때 확인해볼 수 있는 것들","section":"Airbyte","content":" Airbyte 운용 시 디스크 용량이 많이 찼을 때 확인해볼 수 있는 것들 # 개요 # Airbyte를 EC2에서 운영하다보면 어느 시점에 할당한 디스크 용량이 부족한 경우가 발생할 때가 있습니다. 해당 이슈를 탐지하지 못하는 경우에 디스크 용량이 꽉차게 되어 Airbyte가 동작하지 않는 상황이 발생할 수 있습니다. 이 경우 CDC(Change Data Capture)가 정지되기 때문에 미리 대응하는 것이 중요합니다.\nAirbyte 운용 시 디스크 용량을 많이 차지하는 요소 확인 # Airbyte는 Source와 Destination을 연결하는 커넥터를 설정 후 사용하는 방식입니다. 각각은 도커 컨테이너로 구성되어 있고 커넥터의 업데이트 시에는 컨테이너 이미지를 다운로드 받아 사용합니다. 사용자가 설정한 시간에 각 단위 별로 커넥터가 동작하는 것을 Task라 하는데, 동작하는 위치마다 로그가 발생하게 됩니다.\nTask에서 발생하는 리텐션 기간이 30일로 기본값으로 잡혀있습니다. 기본적으로 리텐션 기간 이전까지는 운영 시 로그가 비교적 많이 적재됩니다. 적재 주기와 리텐션 기간을 고려하여 처음에 디스크 용량 산정 시 넉넉하게 주는 것이 좋습니다.\n하지만 한달 이후로 운영하다보면 계속적으로 조금씩 Airbyte 컨테이너 용량이 증가하는 것을 확인하실 수 있습니다. 이를 확인하기 위해서 컨테이너 별 용량을 확인해야 합니다.\ndocker ps --size --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Size}}\u0026#34; 용량을 확인했을 때 많이 차지하는 컨테이너는 airbyte-server 와 airbyte-worker 로 확인했습니다.\nAirbyte 컨테이너 내부 확인 # 이후 컨테이너 내부로 들어가서 어떤 데이터가 용량을 많이 차지하는 지 확인해야 했습니다.\ndocker exec -it airbyte-server /bin/bash airbyte-server에 들어갔을 때 있었던 파일들은 다음과 같습니다. (airbyte-worker 도 상황은 동일하였습니다.)\nls -alh total 200G drwxr-xr-x 1 root root 4.0K Oct 4 02:47 . drwxr-xr-x 1 root root 4.0K Mar 4 2024 .. drwxr-xr-x 1 root root 4.0K Jan 2 1970 airbyte-app -rw-r--r-- 1 root root 200G Oct 4 04:14 build.log drwxr-xr-x 2 root root 4.0K Mar 4 2024 configs -rw------- 1 root root 28M Dec 7 2023 dd-java-agent.jar -rw------- 1 root root 18M Nov 18 2023 opentelemetry-javaagent.jartext 두 컨테이너에서 모두 build.log 파일이 비대하게 적재되어 있는 부분을 확인했습니다. 로그 파일에서는 내부에서 동작하는 부분에 대해서 로그를 남기고 이상 발생 시 트래킹 할 수 있도록 구성되어 있었습니다.\n해당 로그는 운영에 이상을 주기 때문에 S3로 백업 후 삭제하였고, docker compose restart 로 컨테이너를 재부팅하여 삭제된 로그의 캐시를 제거하였습니다.\n한계점 # 사실 위와 같이 작업한다면, 이후 계속적으로 사람이 수작업으로 백업 — 삭제 — 리부팅을 해줘야 합니다. 운영 상 사람으로 발생하는 피해를 줄이기 위해서 자동화를 해주는 것이 좋습니다.\nReference # Understand Airbyte Docker 용량 확인 및 관리 "},{"id":5,"href":"/docs/tech/js7-jobscheduler/configuration/","title":"Configuration","section":"JS7 JobScheduler","content":" JS7 JobScheduler Controller and Agent 초기 접속 설정(w/ docker-compose) # 로그인 및 Controller 설정 # http://localhost:4446/joc/#/login 로 접속합니다. ID, PW ID: root PW: root 접속 후 비밀번호 변경 창이 나오고 변경 진행 Controller URL를 설정하는 창이 나오는데 URL for JOC Cockpit에 http://{controller-docker-name}:4444 로 작성합니다. 클러스터 옵션을 사용하려면 라이센스가 필요합니다. docker ps 를 실행하여 컨테이너 이름를 확인하고, URL에 기입합니다. ex) http://js7-controller-primary-1:4444 접속되며 메인 화면이 나옵니다. Agent 설정 # 1시 방향에 있는 설정 아이콘 \u0026gt; Manage Controllers/Agents 를 클릭합니다. Controller 옆에 있는 … 을 클릭 후 Add Standalone Agent 를 클릭합니다. 팝업 창이 표시되며 각 항목들을 입력합니다. Agent ID 에이전트의 고유 식별자입니다. 고유성은 동일한 컨트롤러에 등록된 모든 에이전트에 적용됩니다. 이 식별자는 나중에 변경할 수 없습니다. Agent Name 에이전트의 이름은 예를 들어 에이전트를 작업에 할당할 때 사용됩니다. 에이전트 ID 와 마찬가지로 에이전트 이름은 고유해야 하지만 나중에 변경할 수 있습니다. Title 검색이 가능한 개별 설명입니다. Alias Names 동일한 에이전트를 다른 이름으로 사용할 수 있습니다. 이는 서로 다른 에이전트 이름을 작업에 할당해야 하는 경우 유용할 수 있습니다. 예를 들어, 프로덕션 환경에서 더 많은 에이전트를 사용하여 비프로덕션 환경에서 더 적은 수의 에이전트에 매핑하는 경우입니다. Process Limit 에이전트는 무제한의 병렬 프로세스를 실행할 수 있습니다. 사용자는 병렬 프로세스를 제한하여 서버 리소스가 고갈되는 것을 방지하고 다음 프로세스가 사용 가능할 때까지 주문을 기다리게 할 수 있습니다. URL 에이전트가 컨트롤러에 접근할 수 있는 프로토콜(HTTP 또는 HTTPS), 호스트 이름 또는 IP 주소 및 포트입니다. http://{agent-docker-name}:4445 형식으로 작성합니다. ex) http://js7-agent-primary-1:4445 Reference # {% embed url=\u0026ldquo; https://qiita.com/saitamanokusa/items/ffb8f05cbc8e75d435ce\" %}\n"},{"id":6,"href":"/docs/tech/aws/aws-lambda/","title":"AWS Lambda 사용 경험 정리","section":"AWS","content":" AWS Lambda 사용 경험 정리 # 개요 # AWS Lambda 를 활용해 스크래핑 및 API를 통해 다양한 데이터를 수집하는 프로젝트를 진행하며 여러 운영 관점에서 중요한 고려 사항들을 직접 체험할 수 있었습니다. Lambda를 사용하면서 마주한 비용 효율성, 리소스 관리, 모니터링, 디버깅 및 AWS의 기타 서비스와의 연계 시 고려해야 할 사항들을 정리해 봤습니다. Lambda 운영 관점에서의 고려 사항 # Lambda를 사용해서 소규모 작업 등을 자동화하여 관리할 때는 비용 및 리소스 관리 효율적입니다. 다만, 파이프라인 규모가 점차 늘어나서 대규모로 작업이 되어야 한다면 EC2 운용 비용보다 비싸지는 지점이 존재합니다. 메모리 총 사용량을 사용자가 지정할 수 있고, 사용한 메모리량에 따라 CPU 스펙과 비용이 최종 결정됩니다. 그렇기 떄문에 100만건 무료라도 많은 양의 메모리를 계속해서 사용한다면 프리티어를 금방 소진할 가능성이 높습니다. Lambda의 모니터링은 AWS Cloudwatch로 하기 때문에 해당 비용 관리도 신경써야 합니다. 로그를 통해서 작업 상태를 기록하는 것은 중요하지만, 많은 양의 로그를 Cloudwatch로 적재하게 된다면 이 또한 비용이 많이 발생하게 됩니다. 그렇기 때문에 개발 후 Lambda를 운영하면서 적절하게 Log가 기록되는 지를 검토해야 합니다. 만약 15분 이상 한 작업에 대해서 Lambda에서 돌려야 한다면 동시성을 증가시켜 최대한 분산 처리를 하는 것을 추천드립니다. 재귀 방식으로 SQS, SNS, S3를 사용해 Lambda를 실행했을 때 최대 16번 이후로 재귀 루프 감지를 하게 됩니다. 이 경우 Lambda에서는 이벤트를 드랍하기 때문에 더 이상 실행할 수 없습니다. 강제로 재귀 루프 감지를 해제할 수는 있지만, 의도치 않은 상황에서 큰 리소스를 사용할 수 있기 때문입니다. 되도록 ARM 아키텍처를 Lambda에서 선택하여 사용하였습니다. 비용 및 성능이 기본적으로 x64 아키텍처보다 좋고, 컴퓨팅 운영을 사용자가 하지 않아도 되기 때문에 다른 서비스에서의 ARM 아키텍처 사용 난이도보다 비교적 낮기 때문입니다. 다만, 사용하는 언어나 비즈니스 로직에서 사용해야 하는 라이브러리 별로 제약 사항이 존재하기 떄문에 테스트 및 검토 후 사용해야 합니다. Python을 사용한 Lambda 함수 작성 시 고려 사항 # Klayer 를 사용하여 Lambda에서 Python 실행 시 필요한 라이브러리를 가져왔습니다. Lambda에서 Python 라이브러리를 사용할 때는 각 함수 별로 계층을 추가해서 사용해야 합니다. 매번 필요한 라이브러리를 정리해서 Lambda 계층을 만드는 것이 번거로운 작업이라 생각했습니다. Lambda에서는 단순한 작업이 주로 이루어지는 것이 좋기 때문에 Klayer에서 제공해주는 라이브러리 내에서 최대한 처리하려고 하였습니다. Python 라이브러리를 직접 Layer에 추가하는 경우는 다음과 같았습니다. 비즈니스 로직에 맞는 Klayer Layer가 없는 경우 Klayer Layer를 사용했는데 250MB Layer 용량 제한에 걸린 경우 AWS Powertools를 사용하여 Lambda에서 로깅이나 모니터링 등 기능들을 추가할 수 있습니다. Lambda를 처음 작업 시에 디버깅이 쉽지 않았던 경험이 있었습니다. 디버깅을 하기 위해 직접 코드를 작성할 필요 없이 AWS Powertool 에서 제공하는 기능을 사용하여 해결하였습니다. AWS SQS(Simple Queue Service)와 연동하여 사용 시 고려 사항 # Lambda에서 SQS 이벤트를 Consuming하는 Window Size에 따라서 처리하는 Lambda의 동시성의 차이가 발생할 수 있습니다. 만약 10개의 이벤트를 SQS로 날리고 Lambda에서의 Window size가 5라면 실행 시간이 충분히 길다 가정하여 2개의 동시성을 가집니다. Lambda의 동시성은 1000이기 떄문에 처리하려는 비즈니스 요구사항에 따라서 Window Size를 조정하여 동시성을 낮춰야 합니다. Window size에 따른 메시지를 처리할 수 있도록 구성해야 하고, 반대로 Window size가 너무 커서 Lambda가 전부 처리하지 못해 타임아웃이 나는 사항을 고려해야 합니다. 공식 문서에서도 이야기되었지만, SQS의 가시성 제한 시간은 함수 타임 아웃의 6배로 설정해야 합니다. 이벤트가 실패할 경우, 재실행을 Lambda에서 할 수 있도록 설정해야 합니다. 네트워크 이슈나 원천 소스가 존재하는 서버의 이상으로 간헐적인 이슈가 발생할 수 있기 때문에 충분한 재실행으로 데이터를 수집할 수 있도록 하였습니다. Lambda 배포 관점에서 고려 사항 # Lambda를 배포할 때 Terraform을 사용하여 배포하였습니다. Lambda를 사용하다 보면 여러 함수들을 생성 및 관리해야 하는 경우가 발생하였고, 이 때마다 계속 함수에 대한 설정을 GUI로 진행했는데 비효율적이였습니다. AWS SAM이나 Serverless Framework 등 Lambda를 개발할 떄 사용할 수 있는 도구들이 존재하였지만 해당 도구들은 POC를 진행 후 사용하지 않기로 결정했습니다. 운영했던 Lambda는 EventBridge Scheduler, SQS 등도 함께 사용하여 데이터 파이프라인을 구성했기 때문에 해당 아키텍처를 함께 배포하거나 내려야 하는 등 요구사항도 존재하여, Terraform을 채택하였습니다. Python 코드와 Terraform 코드를 묶어서 관리하였고, 모노레포로 관리하여 효율성을 높혔습니다. 이렇게 작업했을 경우, Lambda 함수 및 플랫폼이 Git을 통한 버저닝과 CI/CD를 연결이 가능했습니다. "},{"id":7,"href":"/docs/tech/git/gitleaks/","title":"Gitleaks를 사용한 Git 비밀 유출 검사, 사용법","section":"Git","content":" Gitleaks를 사용한 git 비밀 유출 검사, 사용법 # AWS 키가 Github를 통해 노출이 되서 해킹으로 천만원 넘게 과금이 되고, 이를 AWS에서 도와주는 경우를 종종 들었습니다. AWS 키 노출 키워드로 구글에 검색하면 이에 대한 썰(?)을 어렵지 않게 찾아볼 수 있습니다. 키 노출 이슈가 개인이나 기업에서 중대하게 작용하기 때문에 이에 대한 대비책들도 존재합니다.\n이외에도 개발을 하면서 서버나 DB, API 등에 접근하기 위해 비밀 키 등을 사용합니다. 이러한 비밀들이 Git 리포지토리에 커밋이 되었는 지를 확인해주는 오픈소스 도구인 Gitleaks에 대해서 소개해드리려고 합니다.\nGitleaks # Gitleaks는 Git 리포지토리의 비밀번호, API 키, 토큰과 같은 하드코딩 된 비밀을 감지하고 유출을 방지하는 SAST(Static Application Security Testing, 정적 어플리케이션 보안 테스트) 도구입니다.\nGo로 작성되었으며, Homebrew, Docker, Git Repo, Pre-commit, Github-Action 등으로 설치하여 사용할 수 있습니다.\n아래는 Homebrew를 통해서 맥 로컬에 설치하는 방법입니다.\nbrew install gitleaks Gitleaks를 통해서 찾을 수 있는 비밀은 여러가지가 있고, Github Repo에서 각 규칙(총 137가지)들을 확인할 수 있습니다.\n만약, 찾는 규칙이 없다면 오픈소스에 직접 기여해보는 방법도 추천드립니다:)\nGitleaks를 만약 로컬에 설치했다면, 실행하는 방법은 다음과 같습니다.\ngitleaks detect -v # verbose ○ │╲ │ ○ ○ ░ ░ gitleaks 9:28PM INF 157 commits scanned. 9:28PM INF scan completed in 1.49s 9:28PM INF no leaks found 157개의 커밋된 Git Repo를 Gitleaks로 검사했을 때 1.49초만에 완료하고, 찾은 비밀은 없는 경우입니다.\nGitleaks 설정 # Gitleaks로 검사할 때 탬플릿에 목업으로 적어놓은 키 같이 필터링을 해서 비밀키를 찾아야 하는 경우가 있습니다. 이를 위해서 config 파일을 제공합니다. config 파일은 Git Repo가 위치한 루트 패스에 [.gitleaks.toml](https://github.com/gitleaks/gitleaks?tab=readme-ov-file#configuration) 로 저장하면 별도 스크립트 추가없이 실행할 때 자동으로 설정이 적용됩니다.\n.gitleaks.toml을 통해서 설정을 했음에도 불구하고 커밋된 목업 키가 계속 찾아진다면 .gitleaksignore 를 통해서 검사를 무시할 위치를 추가합니다.\nFinding: aws_secret=\u0026#34;AKIAIMNOJVGFDXXXE4OA\u0026#34; RuleID: aws-access-token Secret AKIAIMNOJVGFDXXXE4OA Entropy: 3.65 File: checks_test.go Line: 37 Commit: ec2fc9d6cb0954fb3b57201cf6133c48d8ca0d29 Author: Zachary Rice Email: z@email.com Date: 2018-01-28T17:39:00Z Fingerprint: ec2fc9d6cb0954fb3b57201cf6133c48d8ca0d29:checks_test.go:aws-access-token:37 위처럼 테스트에서 사용하는 키가 Gitleaks 검사 시 나왔고 해당 키는 검사에는 제외하고 싶습니다. 이런 경우, .gitleaksignore에는 Fingerprint의 값을 추가하면 다음 검사부터는 해당 커밋의 키는 검사하지 않게 됩니다.\n# .gitleaksignore ec2fc9d6cb0954fb3b57201cf6133c48d8ca0d29:checks_test.go:aws-access-token:37 gitleaks로 검사를 했을 때 나온 파일들이 이상이 없는 경우, report로 나온 파일을 사용하여서 기준점을 만들 수 있습니다. (해당 기능을 사용하여 리포팅도 가능합니다.)\ngitleaks detect --report-path gitleaks-report.json 재 검사 시 기준점 이후 나온 항목들에 대해서만 추출하는 방식으로 검사할 수 있습니다.\ngitleaks detect --baseline-path gitleaks-report.json --report-path findings.json Case Study: Apache Airflow에 Gitleaks 써보기 # Github에는 수많은 레포들이 존재합니다. Case Study를 위해 클론하여 테스트 해 볼 레포는 데이터 파이프라인을 구축할 때 자주 사용하는 Apache Airflow로 정했습니다. 커밋의 수가 많고, 프로젝트 기간이 오래되어서 확인해보기 좋을 것 같았습니다.\nGitleaks를 사용하기 위해 먼저 Airflow Repo를 클론합니다.\ngit clone https://github.com/apache/airflow.git 작성일 당시 main 브랜치가 25,000 커밋 정도 되었기 때문에 Repo 크기가 큽니다. 클론 시간이 조금 걸릴 수 있습니다.\n이후, CLI(Bash, Zsh…)에서 Gitleaks를 실행합니다.\n○ │╲ │ ○ ○ ░ ░ gitleaks 10:05PM INF 33588 commits scanned. 10:05PM INF scan completed in 21.2s 10:05PM WRN leaks found: 178 브랜치 포함 총 33588 커밋에서 178개의 leaks를 발견했습니다. Verbose 옵션(-v)를 주고 실행하면, 178개에 해당되는 내용이 전부 CLI로 나옵니다. CLI로 나오는 내용을 정리하려면, — report-path 를 주어 json, csv 등으로 출력 가능합니다. 출력한 Report 분석 # 저는 Airflow 검사한 내용을 — report-path 를 사용하여 reports.json으로 출력하고, 분석을 위해 Pandas를 사용하여 Json을 로드하였습니다.\ngitleaks 확인\ngitleaks detect --report-path gitleaks-report.json 나온 Json을 확인해보면 다음과 같은 형태로 나옵니다. (reports.json)\n[ { \u0026#34;Description\u0026#34;: \u0026#34;Detected a Generic API Key, potentially exposing access to various services and sensitive operations.\u0026#34;, \u0026#34;StartLine\u0026#34;: 33, \u0026#34;EndLine\u0026#34;: 33, \u0026#34;StartColumn\u0026#34;: 30, \u0026#34;EndColumn\u0026#34;: 57, \u0026#34;Match\u0026#34;: \u0026#34;Key\\\u0026#34;: \\\u0026#34;cm9tIGlzIHRoZSBraW5n\\\u0026#34;\u0026#34;, \u0026#34;Secret\u0026#34;: \u0026#34;cm9tIGlzIHRoZSBraW5n\u0026#34;, \u0026#34;File\u0026#34;: \u0026#34;helm_tests/other/test_git_ssh_key_secret.py\u0026#34;, \u0026#34;SymlinkFile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Commit\u0026#34;: \u0026#34;f81abd6145d20305148e0a91e8a6ff5026927b70\u0026#34;, \u0026#34;Entropy\u0026#34;: 4.221928, \u0026#34;Author\u0026#34;: \u0026#34;rom sharon\u0026#34;, \u0026#34;Email\u0026#34;: \u0026#34;33751805+romsharon98@users.noreply.github.com\u0026#34;, \u0026#34;Date\u0026#34;: \u0026#34;2024-06-19T06:00:41Z\u0026#34;, \u0026#34;Message\u0026#34;: \u0026#34;add git-sync-ssh secret template (#39936)\u0026#34;, \u0026#34;Tags\u0026#34;: [], \u0026#34;RuleID\u0026#34;: \u0026#34;generic-api-key\u0026#34;, \u0026#34;Fingerprint\u0026#34;: \u0026#34;f81abd6145d20305148e0a91e8a6ff5026927b70:helm_tests/other/test_git_ssh_key_secret.py:generic-api-key:33\u0026#34; }, ... } Pandas로 Json 로드\nimport pandas as pd df = pd.read_json(\u0026#34;./data/gitleaks-report.json\u0026#34;).to_df() df.head() 각각의 항목 값을 집계하여 확인해봅니다.\nRuleID generic-api-key 149 aws-access-token 13 slack-bot-token 7 slack-webhook-url 5 private-key 4 Name: count, dtype: int64 generic-api-key 항목이 많았고, 다음으로 aws-access-token, slack-bot-token, slack-webhook-url, private-key 순으로 있었습니다.\nAirflow Repo에서 확인한 비밀들의 커밋 메시지나 파일들을 보면 대부분 탬플릿이나 예시 파일들에서 사용한 목업 비밀들인 것을 확인할 수 있습니다.\ndf.loc[df[\u0026#34;File\u0026#34;].str.contains(\u0026#34;test\u0026#34;)][\u0026#34;RuleID\u0026#34;].value_counts() ## 출력 결과 RuleID generic-api-key 121 slack-webhook-url 3 private-key 3 slack-bot-token 3 aws-access-token 3 Name: count, dtype: int64 검사로 발견한 178개 중 133개의 항목이 파일이름에 test 가 붇어 있습니다.\n대부분 테스트를 위해서 사용한 목업 형태의 비밀들 임을 확인하였고, 테스트가 붇지 않은 경우에도 howto 나 template, docs, example 등 파일 이름에서 확인할 수 있었습니다.\nGitleaks 자동화 # Gitleaks를 사용한 비밀 검사를 자동하기 위해서 Pre-commit과 Github-Action을 사용합니다.\nPre-commit은 로컬에서 작업 후 커밋에서 코드를 스캔하여 비밀을 감지합니다. Github-Action은 CI/CD 과정에서 커밋이나 PR에서 등록된 비밀을 감지합니다. 아래는 Pre-commit과 Github-Action을 각각 설정하기 위한 코드입니다.\n.pre-commit-config.yaml # repos: - repo: https://github.com/gitleaks/gitleaks rev: v8.16.1 hooks: - id: gitleaks Github-Action # name: gitleaks on: [pull_request, push, workflow_dispatch] jobs: scan: name: gitleaks runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 with: fetch-depth: 0 - uses: gitleaks/gitleaks-action@v2 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} GITLEAKS_LICENSE: ${{ secrets.GITLEAKS_LICENSE}} # Only required for Organizations, not personal accounts. 마치며 # gitleaks를 사용하는 것도 좋지만, 평소에 비밀에 대한 관리가 더 중요하다고 생각합니다. 그렇기 때문에 dotenv 등을 사용해서 내부 환경에 비밀을 로드하여 사용하고, 코드에 비밀이 커밋이 되지 않도록 습관을 들이는 것이 중요합니다.\n그래도 아무리 조심한다 한들 사람이기 때문에 안전장치를 추가하는 것도 좋습니다. gitleaks를 자동화하여 비밀 유출에 개개인이 잘 신경쓰면 좋겠습니다.\n읽어주셔서 감사합니다:)\n더 읽어 보기 # Homepage: https://gitleaks.io/ Github: https://github.com/gitleaks/gitleaks Maintainer: https://blog.gitleaks.io/gitleaks-llc-announcement-d7d06a52e801 https://medium.com/@tarikyegen35/gitleaks-a-secret-scanner-tool-5cd9b6f1d367 https://akashchandwani.medium.com/what-is-gitleaks-and-how-to-use-it-a05f2fb5b034 "},{"id":8,"href":"/docs/tech/langchain/issue-unstructuredurlloader/","title":"UnstructuredURLLoader 무한 로딩 해결","section":"Langchain","content":" UnstructuredURLLoader 무한 로딩 해결 # 요약 # UnstructedURLLoader에서 사용하는 라이브러리 중 unstructured.file_utils.filetype 에서 python-magic 라이브러리를 로드할 때 발생하는 이슈였습니다. 해당 라이브러리를 사용할 떄는 OS 별로 다르게 magic 라이브러리의 종속성을 제공해야 합니다. 그래서 Linux나 MacOS를 사용할 때는 Unstructed 기본 종속성으로 python-magic 이 설치되어 문제가 없지만, Windows를 사용할 떄는 python-magic-bin 을 사용하는 환경에 종속성을 추가해야 합니다. Windows: python-magic-bin Linux, MacOS: python-magic 상황 설명 # Windows 환경에서 Hugging-face Open-Source AI Cookbook 를 보며 LLM 스터디를 진행하고 있었습니다. UnstructuredURLLoader을 사용하여 페이지의 글자들을 가져올 떄 무한 로딩이 걸리는 이슈가 발생하였습니다. 30초 내외로 로드가 완료되어야 정상인데 2분 이상 로드가 완료되지 않는 현상이 발견되었습니다. Ubuntu 환경에서도 현상을 재현해보았지만 큰 문제가 없이 로드가 되어 사용할 수 있었습니다. 예시 코드 # from langchain_community.document_loaders import UnstructuredURLLoader version = \u0026#34;v4.49.0\u0026#34; urls = [ f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/pipeline_tutorial\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/autoclass_tutorial\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/preprocessing\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/training\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/run_scripts\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/tokenizer_summary\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/attention\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/pad_truncation\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/pipeline_webserver\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/tasks_explained\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/hpo_train\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/tasks/sequence_classification\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/tasks/token_classification\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/tasks/question_answering\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/tasks/language_modeling\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/tasks/masked_language_modeling\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/tasks/translation\u0026gt;\u0026#34;, f\u0026#34;\u0026lt;https://huggingface.co/docs/transformers/{version}/ko/tasks/summarization\u0026gt;\u0026#34;, ] loader = UnstructuredURLLoader(urls=urls) docs = loader.load() 접근 방법 # 네트워크가 제한된 망을 사용하고 있던 상황이라 네트워크 이슈라고 생각하여 HTTP 통신을 시도해보았습니다. httpx 를 사용하여 URL 중 하나에 get 요청을 보내어 정상적으로 요청을 받는 지 확인하였습니다. 요청 결과는 정상적으로 받아왔었습니다. 라이브러리 자체의 문제라고 생각하여 Github에 있는 코드들을 확인 및 직접 실행하여 Blocking이 걸리는 부분을 확인하였습니다. UnstructuredURLLoader unstructured.partition.auto.partition unstructured.file_utils.filetype importlib.import_module(\u0026ldquo;magic\u0026rdquo;) importlib 자체는 라이브러리 로드가 되기 떄문에 magic 문제라 생각하고 magic을 실행하였습니다. magic python-magic 라이브러리를 Windows에서 사용할 때 발생하는 원인 등을 조사하였습니다. Windows에서는 python-magic 대신 python-magic-bin 을 사용하여 magic 라이브러리를 사용할 수 있도록 해야 합니다. unstructed issue에 다른 유저가 관련 이슈를 올린 것을 확인하였습니다. https://github.com/Unstructured-IO/unstructured/issues/3438 최종적으로 python-magic-bin==0.4.14 를 설치하였을 떄 정상적으로 작동하였습니다. "},{"id":9,"href":"/docs/tech/python/vscode-python-failed-to-resolve-env/","title":"VSCode Python 익스텐션 에러 트러블 슈팅 (Failed to resolve env)","section":"Python","content":" VSCode Python 익스텐션 에러 트러블 슈팅 (Failed to resolve env) # 문제 상황 # VSCode 내부에서 Intelisense와 Python이 충돌하였다고 알림이 계속 나왔었고 파이썬을 로드하는 상태였습니다. 이로 인해서 Python 관련 VScode 익스텐션이 제대로 동작하지 않았습니다. 문제 해결 방식 # 먼저 VSCode Python 출력창에서 문제가 발생하는 내용을 확인하고 키워드를 통해서 구글링하였습니다. Python Extension: interpreterManager.refresh [l [Error]: Failed to resolve env 구글링을 했을 때 많은 사람들은 Python.Locator을 native에서 js로 변경하는방식으로 해결을 많은 사람들이 하였지만 변경하여도 문제는 계속 발생 하였습니다. https://github.com/microsoft/vscode-python/issues/23956 출력창의 Python 익스텐션 에러를 분석했을 때, 이전에 가상환경으로 사용하고 지웠는데 계속 파이썬 익스텐션에서 로드하고 있었고 이로 인해서 에러를 뱉어내고 있었습니다. 그래서 Python: 캐시 지우기 및 창 다시 로드(Python: Clear Cache and Reload Window) 기능을 사용하여 캐싱을 지웠습니다. 캐싱을 지우니까 정상적으로 익스텐션들을 로드하는 것을 확인할 수 있었습니다. 문제 발생 원인 # 당시 Github Codespace에서 Attention 관련 논문을 읽고 샘플 코드들을 작성하면서 학습하는 과정이였습니다. 그 과정에서 코드에 사용되는 Python 의존성들을 다운받는 과정을 거쳤습니다. 그런데 그 과정에서 Codespace 용량(32G)을 생각하지 않고 pip install을 진행하였습니다. 그래서 다운로드를 계속 받는데 용량을 가득 채우는 상황이 발생하였고, 급하게 종료를 눌렸습니다. 그 과정에서 Python 익스텐션의 내부 동작까지 확인할 수는 없지만, 가상 환경이 계속 캐싱이 되어 있었던 상황이였던거 같고 이후 Codespace를 삭제하였습니다. Codespace로 작업한 동일 Repo를 로드할 때부터 에러는 계속 발생하였습니다. 개선 방향 # VSCode 익스텐션에서 에러가 발생하는 경우가 있으면, 캐싱이 되어 있을 수 있다는 선택지를 생각해야 한다고 생각합니다. 익스텐션에서 캐싱을 지우는 방법을 제공한다면 지우고 다시 확인해보아야 합니다. 인공지능 관련 개발 시 버전 관리는 Github에 저장하되, Google Colab을 활용하여 실험을 할 수 있도록 하는 것이 합리적입니다. 인공지능 관련 작업을 할 때 GPU 리소스를 구글에서 무료로 사용할 수 있게 제공해주기 때문입니다. Codespace는 제한된 리소스 내에서 개발, 테스트를 해볼 수 있는 상황에서 더 활용하는 것이 좋다고 생각합니다. 많은 리소스를 제공하는 것은 아니지만, 무료로독립된 환경에서 간편하게 사용하고 종료할 수 있기 때문에 리소스가 많이 들어가지 않는 개발을 할 때 Codespace를 사용하는게 합리적입니다. "},{"id":10,"href":"/docs/tech/python/python-rocky-linux-installation/","title":"폐쇄망에 Python 설치 (Rocky Linux 8.10, UV 사용)","section":"Python","content":" 폐쇄망에 Python 설치 (Rocky Linux 8.10, UV 사용) # 폐쇄망 환경에 Python 환경을 배포하기 위한 과정을 정리하기 위해서 작성하였습니다. Python 가상 환경 및 Dependency 관리는 UV로 진행합니다. 그래서 UV 설치 과정이 포함되어 있습니다. Rochy-Linux 8.10, Python 3.12.8, UV 0.5.18 기준으로 작성하였습니다. 1. dnf를 사용한 Python 설치 # 1-1. 파일 다운로드 # Python 검색 # dnf를 사용하여 검색할 수 있는 Python 버전은 3.8, 3.9, 3.11, 3.12 입니다. Python 3.10은 dnf search 및 Red Hat 문서에서 확인할 수 없었습니다. dnf search python Python 설치 파일 다운로드 # download-path에 Python을 다운로드 받을 폴더를 지정합니다. dnf install --downloaddir {download-path} --downloadonly python3.12 python3.12-pip python3.12-devel -y 1-2. Python 설치 # 다운로드 받은 파일은 rpm 형태로 구성되어 있습니다. 지정한 download-path에 들어가서 설치 진행합니다. rpm -ivh *.rpm 1-3. Python 설치 확인 # 설치가 되었다면 정상적으로 설치되었는 지 확인합니다. python 명령어로만 실행 시에는 에러가 발생합니다. python3 -V # OK python3.12 -V # OK python # ERROR 2. UV 설치 # 2-1. 파일 다운로드 # Github Repo에 들어가서 UV Release를 확인하고, OS Version에 맞는 설치 파일을 다운로드 받습니다. wget https://github.com/astral-sh/uv/releases/download/0.5.18/uv-x86_64-unknown-linux-gnu.tar.gz 2-2. UV 설치 # 다운로드 받은 UV 파일을 압축 해제하고 해제한 파일들은 shell에서 사용할 수 있도록 조치해야 합니다. 압축 해제 후 보안 이슈가 없다면 /usr/bin 에 파일들을 욺겨서 사용할 수 있도록 합니다. UV 스크립트로 설치하면 $HOME/.local/bin/env에 설치된 파일들이 추가되고 사용하는 shell에 Path 환경 변수를 등록해야 합니다.\ntar -xvf uv-x86_64-unknown-linux-gnu.tar.gz 2-3. UV 설치 확인 # 설치가 완료되었는 지 확인합니다. uv 3. (변외) Python UV 환경 구성 # 설치 뿐만 아니라 UV를 사용하여 Python 환경을 구성하는 과정도 함께 작성합니다.\n3-1. UV 환경 초기화 # uv init {project-name} cd {project-name} 다음과 같이 환경이 구성됩니다.\n{project-name} ├── README.md ├── hello.py ├── pyproject.toml └── uv.lock 3-2. 가상환경 생성 # uv venv를 사용하여 가상 환경을 생성합니다. UV를 사용하여 가상 환경을 생성하면, Python 가상환경을 사전에 설정하지 않더라도 pyproject.toml 을 읽고 Python을 실행시킬 수 있습니다. UV 가상환경 생성 # uv venv UV를 사용하여 초기화 한 프로젝트 내 Python 파일 실행 # uv run hello.py # Hello from {project-name}! "}]